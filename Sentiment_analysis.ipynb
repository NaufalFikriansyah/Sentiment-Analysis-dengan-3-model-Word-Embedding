{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NaufalFikriansyah/Sentiment-Analysis-dengan-3-model-Word-Embedding/blob/main/Sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VWYLq6TUym_j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1fb40d6-62f8-4e12-ff76-44506ddc15f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swifter"
      ],
      "metadata": {
        "id": "OJncs4hdcZae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "90d2cf11-5ad1-4bca-f7e2-dafd99d47bac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting swifter\n",
            "  Downloading swifter-1.3.4.tar.gz (830 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m830.9/830.9 KB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from swifter) (1.3.5)\n",
            "Collecting psutil>=5.6.6\n",
            "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.2/280.2 KB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dask[dataframe]>=2.10.0 in /usr/local/lib/python3.8/dist-packages (from swifter) (2022.2.1)\n",
            "Requirement already satisfied: tqdm>=4.33.0 in /usr/local/lib/python3.8/dist-packages (from swifter) (4.64.1)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.8/dist-packages (from swifter) (7.7.1)\n",
            "Requirement already satisfied: cloudpickle>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from swifter) (2.2.1)\n",
            "Requirement already satisfied: parso>0.4.0 in /usr/local/lib/python3.8/dist-packages (from swifter) (0.8.3)\n",
            "Requirement already satisfied: bleach>=3.1.1 in /usr/local/lib/python3.8/dist-packages (from swifter) (6.0.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from bleach>=3.1.1->swifter) (1.15.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from bleach>=3.1.1->swifter) (0.5.1)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from dask[dataframe]>=2.10.0->swifter) (2023.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.8/dist-packages (from dask[dataframe]>=2.10.0->swifter) (6.0)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.8/dist-packages (from dask[dataframe]>=2.10.0->swifter) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from dask[dataframe]>=2.10.0->swifter) (23.0)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from dask[dataframe]>=2.10.0->swifter) (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.8/dist-packages (from dask[dataframe]>=2.10.0->swifter) (1.21.6)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.0.0->swifter) (5.7.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.0.0->swifter) (7.9.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.0.0->swifter) (0.2.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.0.0->swifter) (3.0.5)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.0.0->swifter) (3.6.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.0.0->swifter) (5.3.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.0->swifter) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.0->swifter) (2.8.2)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (6.0.4)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (6.1.12)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (57.4.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (2.6.1)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (0.2.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (2.0.10)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.8/dist-packages (from partd>=0.3.10->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.8/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (5.7.16)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (0.13.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (1.8.0)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (5.2.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (5.7.3)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (0.16.0)\n",
            "Requirement already satisfied: jinja2<=3.0.0 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (2.11.3)\n",
            "Requirement already satisfied: nbconvert<6.0 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (5.6.1)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (23.2.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (0.2.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect->ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2<=3.0.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (2.0.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.8/dist-packages (from jupyter-core>=4.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (2.6.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (0.6.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (1.5.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (0.4)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.8/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (2.16.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.8/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (4.3.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (5.10.2)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (22.2.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (0.19.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (3.12.0)\n",
            "Building wheels for collected packages: swifter\n",
            "  Building wheel for swifter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for swifter: filename=swifter-1.3.4-py3-none-any.whl size=16321 sha256=10c80bc7afce8508865c50a13ea2124ba6372816b98c8441f5a213ed396f01a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/66/b4/921e351e63d88696932279d6163e125727c9da70ed8ca38419\n",
            "Successfully built swifter\n",
            "Installing collected packages: psutil, jedi, swifter\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "Successfully installed jedi-0.18.2 psutil-5.9.4 swifter-1.3.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kp_QV7x7BEH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2661f6b-12ae-41f2-b152-ce5cc5690f43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install Sastrawi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import re\n",
        "import csv\n",
        "import numpy as np\n",
        "import Sastrawi\n",
        "from sklearn.pipeline import Pipeline\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "nltk.download('punkt')\n",
        "\n",
        "%cd /content/drive/My Drive/Project/Colab Notebooks/"
      ],
      "metadata": {
        "id": "VATx1qyk7IpD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba8df9ad-0763-48a0-a9a1-01d7d5dd3ee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/My Drive/Project/Colab Notebooks/sentimen-analysis'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#baca data\n",
        "df = pd.read_csv(\"label.csv\", error_bad_lines=False,encoding='ISO-8859-1')"
      ],
      "metadata": {
        "id": "YKRtKxiQ7T1Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "819ddd98-afcd-499d-b779-5d7c76b73d51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3326: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-96ef3a2e8610>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#baca data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ISO-8859-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'label.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "m9pPKumX7WKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for row in reader:\n"
      ],
      "metadata": {
        "id": "Lx2e7wnRz37C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#proses casefolding\n",
        "import re\n",
        "def casefolding(Comment):\n",
        "  Comment = Comment.lower()\n",
        "  Comment = Comment.strip(\" \")\n",
        "  return Comment\n",
        "df['Comment'] = df['Comment'].apply(casefolding)"
      ],
      "metadata": {
        "id": "DwRjVnoT7Y42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['Comment'])"
      ],
      "metadata": {
        "id": "PAcTr1b27b38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#proses menghilangkan simbol dan emoji\n",
        "def remove_text_special (text):\n",
        "  text = text.replace('\\\\t',\"\").replace('\\\\n',\"\").replace('\\\\u',\"\").replace('\\\\',\"\")\n",
        "  text = text.encode('ascii', 'replace').decode('ascii')\n",
        "  return text.replace(\"http://\",\" \").replace(\"https://\", \" \")\n",
        "df['Comment'] = df['Comment'].apply(remove_text_special)\n",
        "print(df['Comment'])"
      ],
      "metadata": {
        "id": "VwEjqG4o7tMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#proses menghilangkan angka\n",
        "def remove_numbers (text):\n",
        "  return re.sub(r\"\\d+\", \"\", text)\n",
        "df['Comment'] = df['Comment'].apply(remove_numbers)\n",
        "print(df['Comment'])"
      ],
      "metadata": {
        "id": "p4hbnQSB70yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation"
      ],
      "metadata": {
        "id": "TX56J1zY8BeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_tanda_baca(text):\n",
        "  text = text.replace(\".\",\"\")\n",
        "  text = text.replace(\",\",\"\")\n",
        "  text = text.replace(\"'\",\"\")\n",
        "  text = text.replace('\"',\"\")\n",
        "  text = text.replace(\"&\",\"\")\n",
        "  text = text.replace(\"-\",\"\")\n",
        "  text = text.replace(\"?\",\"\")\n",
        "  text = text.replace(\"!\",\"\")\n",
        "  text = text.replace(\"(\",\"\")\n",
        "  text = text.replace(\")\",\"\")\n",
        "  text = text.replace(\"/\",\"\")\n",
        "  text = text.replace(\"#\",\"\")\n",
        "  text = text.replace(\"%\",\"\")\n",
        "  text = text.replace(\"*\",\"\")\n",
        "  text = text.replace(\":\",\"\")\n",
        "  text = text.replace(\";\",\"\")\n",
        "  text = text.replace(\"@\",\"\")\n",
        "  text = text.replace(\"_\",\"\")\n",
        "  text = text.replace(\"+\",\"\")\n",
        "  text = text.replace(\">\",\"\")\n",
        "  text = text.replace(\"<\",\"\")\n",
        "  text = text.replace(\"$\",\"\")\n",
        "  return text"
      ],
      "metadata": {
        "id": "dz5daMNy8D0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Comment'] = df['Comment'].apply(remove_tanda_baca)"
      ],
      "metadata": {
        "id": "SW0Co_Tj8Gxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['Comment'])"
      ],
      "metadata": {
        "id": "GqyaNUVY9w_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#proses tokenizing\n",
        "def word_tokenize_wrapper(text):\n",
        "  return word_tokenize(text)\n",
        "df['review_token'] = df['Comment'].apply(word_tokenize_wrapper)\n",
        "print(df['review_token'])"
      ],
      "metadata": {
        "id": "7DajVTcNBm_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalisasi\n",
        "normalize = pd.read_excel(\"Normalization Data.xlsx\")\n",
        "\n",
        "normalize_word_dict = {}\n",
        "\n",
        "for index, row in normalize.iterrows():\n",
        "  if row[0] not in normalize_word_dict:\n",
        "    normalize_word_dict[row[0]] = row[1]\n",
        "\n",
        "def normalized_term(comment):\n",
        "  return [normalize_word_dict[term] if term in normalize_word_dict else term for term in comment]\n",
        "\n",
        "df['comment_normalize'] = df['review_token'].apply(normalized_term)\n",
        "df['comment_normalize'].head(20)"
      ],
      "metadata": {
        "id": "CrXz8L2X_-7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['comment_normalize']"
      ],
      "metadata": {
        "id": "m4wIqbONAJsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stopword Removal\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "txt_stopwords = stopwords.words('indonesian')\n",
        "\n",
        "def stopwords_removal(filtering) :\n",
        "  filtering = [word for word in filtering if word not in txt_stopwords]\n",
        "  return filtering\n",
        "\n",
        "df['stopwords_removal'] = df['comment_normalize'].apply(stopwords_removal)\n",
        "df['stopwords_removal']"
      ],
      "metadata": {
        "id": "eBlUaMz1mTvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_stopwords = pd.read_csv(\"list_stopwords.csv\")\n",
        "print(data_stopwords)\n",
        "\n",
        "\n",
        "def stopwords_removal2(filter) :\n",
        "  filter = [word for word in filter if word not in data_stopwords]\n",
        "  return filter\n",
        "\n",
        "df['stopwords_removal_final'] = df['stopwords_removal'].apply(stopwords_removal2)\n",
        "df['stopwords_removal_final']"
      ],
      "metadata": {
        "id": "mPATNd71TXZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "import string\n",
        "import swifter\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()"
      ],
      "metadata": {
        "id": "pazvQNbJbka2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stemming (term):\n",
        "  return stemmer.stem(term)"
      ],
      "metadata": {
        "id": "VvWsrXusbsFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "term_dict = {}"
      ],
      "metadata": {
        "id": "iwy5wsSpbxUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for document in df['stopwords_removal_final']:\n",
        "  for term in document:\n",
        "    if term not in term_dict:\n",
        "      term_dict[term] = ''"
      ],
      "metadata": {
        "id": "qx9twDaHb8GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(term_dict))\n",
        "print(\"-----------------------------\")"
      ],
      "metadata": {
        "id": "YEt8KCgBdG4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for term in term_dict:\n",
        "  term_dict[term] = stemming(term)\n",
        "  print(term,\":\",term_dict[term])\n",
        "\n",
        "print(term_dict)\n",
        "print(\"-----------------------------\")"
      ],
      "metadata": {
        "id": "OHWtVMZ-dP3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stemming(document):\n",
        "  return [term_dict[term] for term in document]"
      ],
      "metadata": {
        "id": "v4Xn4XiIiwXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['stemming'] = df['stopwords_removal_final'].swifter.apply(get_stemming)"
      ],
      "metadata": {
        "id": "lHAl37YCh9dI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['stemming'])"
      ],
      "metadata": {
        "id": "g5Bs-vjmiQ_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()\n",
        "df"
      ],
      "metadata": {
        "id": "Z-eTWxGc8CsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "df.to_csv('hasil_stemming.csv')"
      ],
      "metadata": {
        "id": "89h-S_t7Z8n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting Data 80% Training dan 20% Testing"
      ],
      "metadata": {
        "id": "y0f8LAqQmUo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Train Test Split Function\n",
        "X = df[['Comment', 'Rating', 'stemming']]\n",
        "y = df['Label']\n",
        "def split_train_test(data_df, test_size=0.2, shuffle_state=True): # 80% Training & 20% Testing. Ubah nilai pada variable test_size jika ingin skala yang lain\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, \n",
        "                                                        y,\n",
        "                                                        shuffle=shuffle_state,\n",
        "                                                        test_size=test_size, \n",
        "                                                        stratify=y,\n",
        "                                                        random_state=15)\n",
        "    print(\"Value counts for Train sentiments\")\n",
        "    print(Y_train.value_counts())\n",
        "    print(\"Value counts for Test sentiments\")\n",
        "    print(Y_test.value_counts())\n",
        "    print(type(X_train))\n",
        "    print(type(Y_train))\n",
        "    X_train = X_train.reset_index()\n",
        "    X_test = X_test.reset_index()\n",
        "    Y_train = Y_train.to_frame()\n",
        "    Y_train = Y_train.reset_index()\n",
        "    Y_test = Y_test.to_frame()\n",
        "    Y_test = Y_test.reset_index()\n",
        "    print(X_train.head())\n",
        "    return X_train, X_test, Y_train, Y_test\n",
        "\n",
        "# Call the train_test_split\n",
        "X_train, X_test, Y_train, Y_test = split_train_test(df)"
      ],
      "metadata": {
        "id": "2qSiXpxh1W8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "  \n",
        "# 1 = Positive, 0 = Negative\n",
        "dataPlot = {'Positive':Y_train['Label'].value_counts()[1], 'Negative':Y_train['Label'].value_counts()[0] }\n",
        "labelx = list(dataPlot.keys())\n",
        "values = list(dataPlot.values())\n",
        "  \n",
        "fig = plt.figure(figsize = (10, 5))\n",
        " \n",
        "# creating the bar plot\n",
        "plt.bar(labelx, values, color ='maroon',\n",
        "        width = 0.4)\n",
        " \n",
        "plt.ylabel(\"Jumlah Data\")\n",
        "plt.xlabel(\"Sentiment\")\n",
        "plt.title(\"Sentimen Training\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DeDCjcUWmxJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 = Positive, 0 = Negative\n",
        "dataPlot = {'Positive':Y_test['Label'].value_counts()[1], 'Negative':Y_test['Label'].value_counts()[0]}\n",
        "labelx = list(dataPlot.keys())\n",
        "values = list(dataPlot.values())\n",
        "  \n",
        "fig = plt.figure(figsize = (10, 5))\n",
        " \n",
        "# creating the bar plot\n",
        "plt.bar(labelx, values, color ='maroon',\n",
        "        width = 0.4)\n",
        " \n",
        "plt.xlabel(\"Jumlah Data\")\n",
        "plt.ylabel(\"Sentiment\")\n",
        "plt.title(\"Sentimen Testing\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YJPsLhDNnWeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Menerapkan Word Embedding : Word2Vex "
      ],
      "metadata": {
        "id": "H7VQNbVepYKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refrensi:\n",
        "https://medium.com/swlh/sentiment-classification-using-word-embeddings-word2vec-aedf28fbb8ca"
      ],
      "metadata": {
        "id": "B77Qk0SEpm1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import time\n",
        "\n",
        "OUTPUT_FOLDER = '/content/drive/My Drive/Project' #Untuk menyimpan file model, sesuaikan dengan drive sendiri. \n",
        "# Skip-gram model (sg = 1)\n",
        "size = 1000\n",
        "window = 3\n",
        "min_count = 1\n",
        "workers = 3\n",
        "sg = 1\n",
        "\n",
        "word2vec_model_file = OUTPUT_FOLDER + 'word2vec_' + str(size) + '.model'\n",
        "start_time = time.time()\n",
        "stemmed_tokens = pd.Series(df['stemming']).values\n",
        "# Train the Word2Vec Model\n",
        "w2v_model = Word2Vec(stemmed_tokens, min_count = min_count, size = size, workers = workers, window = window, sg = sg)\n",
        "print(\"Time taken to train word2vec model: \" + str(time.time() - start_time))\n",
        "w2v_model.save(word2vec_model_file)"
      ],
      "metadata": {
        "id": "8LpJoN7r9M_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Load the model from the model file\n",
        "sg_w2v_model = Word2Vec.load(word2vec_model_file)\n",
        "# Unique ID of the word\n",
        "print(\"Index of the word 'bagus':\")\n",
        "print(sg_w2v_model.wv.vocab[\"bagus\"].index)\n",
        "# Total number of the words \n",
        "print(len(sg_w2v_model.wv.vocab))\n",
        "# Print the size of the word2vec vector for one word\n",
        "print(\"Length of the vector generated for a word\")\n",
        "print(len(sg_w2v_model['bagus']))\n",
        "# Get the mean for the vectors for an example review\n",
        "print(\"Print the length after taking average of all word vectors in a sentence:\")\n",
        "print(np.mean([sg_w2v_model[token] for token in df['stemming'][0]], axis=0))"
      ],
      "metadata": {
        "id": "V8WC3VfR-AMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_filename = OUTPUT_FOLDER + 'train_review_word2vec.csv'\n",
        "with open(word2vec_filename, 'w+') as word2vec_file:\n",
        "    for index, row in X_train.iterrows():\n",
        "        model_vector = (np.mean([sg_w2v_model[token] for token in row['stemming']], axis=0)).tolist()\n",
        "        if index == 0:\n",
        "            header = \",\".join(str(ele) for ele in range(1000))\n",
        "            word2vec_file.write(header)\n",
        "            word2vec_file.write(\"\\n\")\n",
        "        # Check if the line exists else it is vector of zeros\n",
        "        if type(model_vector) is list:  \n",
        "            line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n",
        "        else:\n",
        "            line1 = \",\".join([str(0) for i in range(1000)])\n",
        "        word2vec_file.write(line1)\n",
        "        word2vec_file.write('\\n')"
      ],
      "metadata": {
        "id": "U3LuK1Yp-A3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Menerapkan model Support Vector Machine (SVM) untuk Word2vec"
      ],
      "metadata": {
        "id": "4vE2gNKcp0Xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "from sklearn import svm\n",
        "# Load from the filename\n",
        "word2vec_df = pd.read_csv(word2vec_filename)\n",
        "#Initialize the model\n",
        "clf_decision_word2vec = svm.LinearSVC()\n",
        "\n",
        "start_time = time.time()\n",
        "# Fit the model\n",
        "clf_decision_word2vec.fit(word2vec_df, Y_train['Label'])\n",
        "print(\"Time taken to fit the model with word2vec vectors: \" + str(time.time() - start_time))"
      ],
      "metadata": {
        "id": "4Mvt-6AQ-OGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_df"
      ],
      "metadata": {
        "id": "y-9HdoOdy1Zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluasi model SVM untuk Word2vec"
      ],
      "metadata": {
        "id": "LnY4fsrBqAOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "test_features_word2vec = []\n",
        "for index, row in X_test.iterrows():\n",
        "    model_vector = np.mean([sg_w2v_model[token] for token in row['stemming']], axis=0)\n",
        "    if type(model_vector) is list:\n",
        "        test_features_word2vec.append(model_vector)\n",
        "    else:\n",
        "        test_features_word2vec.append(np.array([0 for i in range(1000)]))\n",
        "test_predictions_word2vec = clf_decision_word2vec.predict(test_features_word2vec)\n",
        "print(classification_report(Y_test['Label'],test_predictions_word2vec))"
      ],
      "metadata": {
        "id": "Mh6rnCWJ-g-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "from sklearn import metrics\n",
        "\n",
        "actual = Y_test['Label']\n",
        "predicted = test_predictions_word2vec\n",
        "\n",
        "confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
        "\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
        "\n",
        "cm_display.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "25xK7LwlybqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FASTTEXT"
      ],
      "metadata": {
        "id": "_y9ddrmOQ2Sm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/fasttext-sentiment-analysis-for-tweets-a-straightforward-guide-9a8c070449a2"
      ],
      "metadata": {
        "id": "u3Oi0qOuQ85R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://thinkinfi.com/fasttext-word-embeddings-python-implementation/"
      ],
      "metadata": {
        "id": "zBEWvz3JpQHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.fasttext import FastText\n",
        "\n",
        "# Defining values for parameters\n",
        "embedding_size = 1000\n",
        "window_size = 3\n",
        "min_word = 1\n",
        "down_sampling = 1e-2\n",
        "\n",
        "\n",
        "fast_Text_model = FastText(stemmed_tokens,\n",
        "                      size=embedding_size,\n",
        "                      window=window_size,\n",
        "                      min_count=min_word,\n",
        "                      sample=down_sampling,\n",
        "                      workers = 3,\n",
        "                      sg=1,\n",
        "                      iter=100)"
      ],
      "metadata": {
        "id": "5cVh-BNSoGsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fastText_filename = OUTPUT_FOLDER + 'ft_model_yelp' + str(size) + '.model'\n",
        "# Save fastText gensim model\n",
        "start_time = time.time()\n",
        "fast_Text_model.save(fastText_filename)\n",
        "# Load saved gensim fastText model\n",
        "fast_Text_model = FastText.load(fastText_filename)\n",
        "\n",
        "print(\"Time taken to train word2vec model: \" + str(time.time() - start_time))\n",
        "fast_Text_model.wv['bagus']"
      ],
      "metadata": {
        "id": "VikEItNSoveh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(fastText_filename, 'w+') as fastText_file:\n",
        "    for index, row in X_train.iterrows():\n",
        "        model_vector = (np.mean([fast_Text_model[token] for token in row['stemming']], axis=0)).tolist()\n",
        "        if index == 0:\n",
        "            header = \",\".join(str(ele) for ele in range(1000))\n",
        "            fastText_file.write(header)\n",
        "            fastText_file.write(\"\\n\")\n",
        "        # Check if the line exists else it is vector of zeros\n",
        "        if type(model_vector) is list:  \n",
        "            line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n",
        "        else:\n",
        "            line1 = \",\".join([str(0) for i in range(1000)])\n",
        "        fastText_file.write(line1)\n",
        "        fastText_file.write('\\n')"
      ],
      "metadata": {
        "id": "4Qo1ka6QsutN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fastText_df = pd.read_csv(fastText_filename)\n",
        "fastText_df = fastText_df.dropna(axis=1)\n",
        "fastText_df"
      ],
      "metadata": {
        "id": "1Qur2sv_uwhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "clf_decision_fastText = svm.LinearSVC()\n",
        "clf_decision_fastText = clf_decision_fastText.fit(fastText_df, Y_train['Label'])\n",
        "print(\"Time taken to fit the model with fastText vectors: \" + str(time.time() - start_time))\n",
        "\n",
        "test_features_fastText = []\n",
        "for index, row in X_test.iterrows():\n",
        "    model_vector = np.mean([fast_Text_model[token] for token in row['stemming']], axis=0)\n",
        "    if type(model_vector) is list:\n",
        "        test_features_fastText.append(model_vector)\n",
        "    else:\n",
        "        test_features_fastText.append(np.array([0 for i in range(1000)]))\n",
        "test_predictions_fastText = clf_decision_fastText.predict(test_features_fastText)\n",
        "print(classification_report(Y_test['Label'],test_predictions_fastText))"
      ],
      "metadata": {
        "id": "QLCl2LS_Cilw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "from sklearn import metrics\n",
        "\n",
        "actual = Y_test['Label']\n",
        "predicted = test_predictions_fastText\n",
        "\n",
        "confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
        "\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
        "\n",
        "cm_display.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gZWB1TUmEDTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://stackoverflow.com/questions/54589669/confusion-matrix-error-classification-metrics-cant-handle-a-mix-of-multilabel"
      ],
      "metadata": {
        "id": "iwF_A1lcyqjF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GLOVE"
      ],
      "metadata": {
        "id": "jFHurdZqzp8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.cs.toronto.edu/~lczhang/aps360_20191/lec/w06/sentiment.html"
      ],
      "metadata": {
        "id": "zXNVjJKCidDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://rifqifai.com/membuat-model-glove-dari-korpus-wikipedia-bahasa-indonesia/"
      ],
      "metadata": {
        "id": "7OvyDMtLO_Hu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://yudiwbs.wordpress.com/2018/04/02/glove-untuk-wikipedia-bahasa-indonesia/"
      ],
      "metadata": {
        "id": "3LIhJC_bMSoQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://colab.research.google.com/drive/19Ghc_Q21-k3n4ma8UuoRGXzGFeRN4qKB#scrollTo=XNvMFEolJuSi"
      ],
      "metadata": {
        "id": "r4oLDdNi_2Ou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.sunnyville.ai/custom-class-glove-embeddings-sklearn-pipeline/"
      ],
      "metadata": {
        "id": "iob6BQrTIf_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://bagas.me/spacy-bahasa-indonesia.html"
      ],
      "metadata": {
        "id": "vAsTf05vO5a6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "import spacy\n",
        "nlp = spacy.blank(\"id\")\n",
        "\n"
      ],
      "metadata": {
        "id": "zvJ_qYeIIQS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Vectors for the text column and dropping the rest\n",
        "column_preprocessor = ColumnTransformer(\n",
        "    [\n",
        "        ('text_tfidf', TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}'), 'Comment'),\n",
        "    ],\n",
        "    remainder='drop',\n",
        "    n_jobs=1\n",
        ")\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('column_preprocessor', column_preprocessor),\n",
        "    ('svm', svm.LinearSVC())\n",
        "])\n",
        "\n",
        "# Training\n",
        "pipeline.fit(X_train, Y_train['Label'])\n",
        "predictions = pipeline.predict(X_test)\n",
        "print(classification_report(Y_test['Label'], predictions))"
      ],
      "metadata": {
        "id": "yiFCeqn1TOv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GloveVectorTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, nlp):\n",
        "        self.nlp = nlp\n",
        "        self.dim = 300\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([self.nlp(text).vector for text in X])"
      ],
      "metadata": {
        "id": "62MEPtR6Wy62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''column_preprocessor = ColumnTransformer(\n",
        "    [\n",
        "        ('text_glove', GloveVectorTransformer(nlp), 'Comment'),\n",
        "    ],\n",
        "    remainder='drop',\n",
        "    n_jobs=1\n",
        ")\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('column_preprocessor', column_preprocessor),\n",
        "    ('svm', svm.LinearSVC())\n",
        "])\n",
        "\n",
        "# Training\n",
        "pipeline.fit(X_train, Y_train)\n",
        "predictions = pipeline.predict(X_test)\n",
        "print(classification_report(Y_test['Label'], predictions))'''"
      ],
      "metadata": {
        "id": "ZDMCkPbXeKCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        " \n",
        "namaFileGlove = \"/content/drive/MyDrive/Project/sentimen-analysis/glove_50dim_wiki.id.case.text.txt\"\n",
        "glove_file = datapath(namaFileGlove)\n",
        "tmp_file = get_tmpfile(\"w2vec_glove_wiki_id.txt\")\n",
        " \n",
        "glove2word2vec(glove_file, tmp_file)\n",
        "\n"
      ],
      "metadata": {
        "id": "dbRTfS6xE6aM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "gloveModel = KeyedVectors.load_word2vec_format(tmp_file, binary=False)\n",
        "hasil = gloveModel.most_similar(\"keren\")\n",
        "print(\"keren:{}\".format(hasil))\n",
        "hasil = gloveModel.most_similar(\"tempo\")\n",
        "\n",
        "sim = gloveModel.similarity(\"enak\", \"bagus\")\n",
        "print(\"Kedekatan enak-bagus: {}\".format(sim))\n",
        "\n",
        "hasil = gloveModel.most_similar_cosmul(positive=['perempuan', 'raja'], negative=['pria'], topn=1)\n",
        "print(\"pria-raja, perempuan-?: {}\".format(hasil))\n"
      ],
      "metadata": {
        "id": "0jriRn_lLYVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vector kata bagus\n",
        "print(\"Time taken to train Glove model: \" + str(time.time() - start_time))\n",
        "\n",
        "gloveModel['langganan']"
      ],
      "metadata": {
        "id": "PnlmXQl7L9bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(gloveModel)"
      ],
      "metadata": {
        "id": "zX0tjSOib0hY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_features_glove = []\n",
        "for token in X_train:\n",
        "  test_features_glove.append(gloveModel[token])\n",
        "\n",
        "test_features_glove"
      ],
      "metadata": {
        "id": "E797LpRYc8gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_features_glove"
      ],
      "metadata": {
        "id": "80LgQ_vwxo6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_decision_glove = svm.LinearSVC()\n",
        "clf_decision_glove = clf_decision_glove.fit(test_features_glove, Y_train['Label'])\n",
        "test_predictions_glove = clf_decision_glove.predict(test_features_glove)\n",
        "print(classification_report(Y_test['Label'],test_predictions_glove))"
      ],
      "metadata": {
        "id": "qEoTtzM_fGky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual = Y_test['Label']\n",
        "predicted = test_features_glove\n",
        "\n",
        "confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
        "\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
        "\n",
        "cm_display.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E5EWuFlTOLYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ieAMi7s5fBNg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}